{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Step 1: Main Dataset",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport scipy.stats as stats\nfrom statsmodels.stats.proportion import proportions_ztest",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Load datasets\ndf_raw = pd.read_csv(\"https://code.s3.yandex.net/datasets/gift.csv\")\ndf_raw_description = pd.read_csv(\"https://code.s3.yandex.net/python-for-analytics/gift_entry.csv', sep=';\")\n\n# Create copies of the datasets to work with\ndf = df_raw.copy()\ndf_description = df_raw_description.copy()\n\n# Examine general information about the main dataset\ndf.info()\ndf.describe()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Look at 10 random rows from the main dataset\ndf.sample(10)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Look at 10 random rows from the supplementary dataset\ndf_description.sample(10)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "After reviewing the data, several conclusions can be drawn\n\n1) Negative values are observed in the customer_id column\n\n2) Negative values are also observed in the quanity column. Quantity also cannot have negative indicators\n\n3) Negative values are also observed in the price column. The price also cannot have negative indicators\n\n4) In the quantity column we observe an abnormally high value (80995). If we compare it with the average or medial value, we have an excess of several hundred times\n\n5) There are no gaps",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Checking for gaps",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Check for missing values in the main dataset\nprint(\"Main dataset - missing values:\")\nprint(df.isnull().sum())\n\n# Check for missing values in the dataset with record descriptions\nprint(\"\\nDataset with record descriptions - missing values:\")\nprint(df_description.isnull().sum())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "There was one gap in the dataset with a description - we delete it",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Drop rows with missing values in the dataset with record descriptions\ndf_description = df_description.dropna()\nprint(\"\\nDataset with record descriptions - missing values removed:\")\nprint(df_description.isnull().sum())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Checking for duplicates",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Check for duplicates in the main dataset\nprint(\"Main dataset - number of duplicates:\")\nprint(df.duplicated().sum())\n\n# Check for explicit duplicates in df_description\nduplicate_count = df_description[df_description['entry'].str.lower().duplicated()].shape[0]\nprint(\"\\nDataset with record descriptions - number of duplicates:\")\nprint(duplicate_count)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "There are obvious duplicates in the main dataset (3573 pieces) in the main dataset and (3 pieces) in the description dataset. We also delete them",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Remove duplicates in the main dataset\ndf = df.drop_duplicates()\n\n# Remove duplicates in the dataset with record descriptions\n\n# Create a temporary copy of the 'entry' column converted to lowercase\ntemp_lower_entry = df_description['entry'].str.lower()\n\n# Identify indices of duplicates\nduplicate_indices = temp_lower_entry[temp_lower_entry.duplicated()].index\n\n# Remove duplicates from the main dataframe\ndf_description_cleaned = df_description.drop(duplicate_indices)\n\n# Check for duplicates after removal\nprint(\"\\nNumber of duplicates removed from the main dataset:\")\nprint(len(df_raw) - len(df))\n\n# Output number of duplicates\nprint(\"\\nNumber of duplicates removed from the description dataset:\")\nprint(len(duplicate_indices))\n\n# Calculate the percentage of removed rows and explicit duplicates in the main dataset\npercentage_removed = ((len(df_raw) - len(df)) / len(df_raw)) * 100\n\nprint(\"\\nPercentage of rows removed from the main dataset:\")\nprint(f\"{percentage_removed:.4f}%\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We removed 1% of rows as part of data preprocessing from the main dataset. In the dataset with the description, only 4 lines were removed.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Checking datasets for matching ID numbers",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Check the presence of identification numbers in both dataframes\nmain_ids = set(df['entry_id'])\ndescription_ids = set(df_description['entry_id'])\n\n# Find common identification numbers\ncommon_ids = main_ids.intersection(description_ids)\n\n# Find identification numbers that are only in the main dataset\nonly_in_main = main_ids - description_ids\n\n# Find identification numbers that are only in the description dataset\nonly_in_description = description_ids - main_ids\n\nprint(f\"Total number of identification numbers in the main dataset: {len(main_ids)}\")\nprint(f\"Total number of identification numbers in the description dataset: {len(description_ids)}\")\nprint(f\"Number of common identification numbers: {len(common_ids)}\")\nprint(f\"Number of identification numbers only in the main dataset: {len(only_in_main)}\")\nprint(f\"Number of identification numbers only in the description dataset: {len(only_in_description)}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Merging two datasets",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Merge the two datasets into one (using entry_id as the key)\ndf_full = df.merge(df_description, on='entry_id', how='outer')\n\n# Create a copy of the merged dataset\ndf_full_raw = df_full.copy()\n\n# View the main information about the merged dataset\ndf_full.info()\n\n# Count the number of missing values in each column\nprint('Number of missing values per column:')\nprint(df_full.isna().sum())\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "There are Gaps in the entry column (product name). We are going to study them and decide what to do with them.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Exploring gaps and \"strange\" values",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create a separate dataframe that includes rows with missing 'entry' values\nfiltered_df = df_full[df_full['entry'].isna()]\n\n# Examine the dataframe with missing values\nfiltered_df.describe()\n\n# Calculate the proportion of missing values in the entire dataset\nshare_of_skips = len(filtered_df) / len(df_full)\nprint(f\"Proportion of missing values: {share_of_skips:.4f}%\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We see that all data has the same customer_id (equal to -1) and the price is not indicated (equal to zero everywhere). Since there is little data with gaps (less than 1%), it was decided to delete them.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\n# Remove rows with missing 'entry' values\ndf_full.dropna(subset=['entry'], inplace=True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Check the result:\nprint('Number of missing values per column:')\nprint(df_full.isna().sum())\nprint(\"Number of rows after removing missing values:\")\nprint(len(df_full))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_full.describe()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Let's examine the columns customer_id (there are fields with the value -1), quantity (negative values), price (negative values)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Examine rows where 'customer_id' equals -1\ndf_full.query('customer_id == -1').sample(10)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# View statistics for rows where 'customer_id' equals -1\ndf_full.query('customer_id == -1').describe()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "It seems that some orders went under customer_id = -1, which either by mistake did not receive “their” identification number or there was some error in the data. In any case, some of them are real orders and I am not going to delete them. But I will separately consider with values ​​quantity < 0",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Examine rows where 'quantity' has a negative value\ndf_full_quantity = df_full.query('quantity < 0')\ndf_full_quantity",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Additionally, check the descriptions\nentry_unique = df_full_quantity['entry'].unique()\nprint(entry_unique)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "\n# Ensure that the price is 0 for these rows\ndf_full_quantity.describe()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Apparently we are talking about a written-off product or various types of errors. The price for all is 0, so we can safely delete these rows.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Update the merged dataset to only include rows with positive 'quantity' values\ndf_full = df_full.query('quantity > 0')\ndf_full.describe()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Examine rows where 'price' has a negative value\ndf_full_price = df_full.query('price < 0')\ndf_full_price",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We see only 2 rows where we have negative values ​​in the price column. We are talking about some kind of adjustment. The data will not be needed for future analysis. Delete them",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Update the merged dataset to only include rows with positive 'price' values\ndf_full = df_full.query('price > 0')\ndf_full.describe()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Check for values written in uppercase\nuppercase_entries = df_full[df_full['entry'].str.isupper()]\n\nprint(f\"\\nNumber of uppercase values in the 'entry' column: {len(uppercase_entries)}\")\nif len(uppercase_entries) > 0:\n    print(\"Uppercase values:\")\n    print(uppercase_entries)\n\n    # Get unique uppercase values\n    unique_uppercase_entries = uppercase_entries['entry'].unique()\n\n    print(f\"\\nUnique uppercase values in the 'entry' column: {len(unique_uppercase_entries)}\")\n    print(unique_uppercase_entries)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Identify indices of duplicates\ndeleted_indices_1 = uppercase_entries.index\n\n# Remove duplicates from the main dataframe\ndf_full = df_full.drop(deleted_indices_1)\n\nprint(f\"\\nNumber of duplicates removed: {len(deleted_indices_1)}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Values ​​in the entry column that are written in uppercase are not products. We are talking about various kinds of adjustments and additional actions of the online store (postage, management, commission, and so on). Decided to remove them",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Calculate the percentage of removed data\ndeleted_data_1 = ((len(df_full_raw) - len(df_full)) / len(df_full_raw)) * 100\nprint(f\"Percentage of removed data: {deleted_data_1:.4f}%\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Step 2: Pre-processing and start of exploratory analysis",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "I have already removed negative values ​​(as well as 0) in the price and quantity columns in the previous step. Now let's look at the abnormally high values ​​(outliers) and decide what to do with them",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df_full.describe()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The quantity and price columns contain abnormally high values. Let's build graphs to clearly see emissions",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Checking the quantity column",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Set the size of the boxplot\nplt.figure(figsize=(10, 6))\n\n# Plot a horizontal boxplot\nplt.boxplot(x = df_full['quantity'],  # quantity data\n            vert=False,     # horizontal boxplot\n            showmeans=True, # show mean on the plot\n            meanline=True,  # show mean as a line\n            patch_artist=True,  # fill the boxplot with color\n            # set outliers color to red\n            flierprops=dict(markerfacecolor='red')) \n\n# Add title to the plot\nplt.title(\"Outlier Analysis of Quantity\")\n\n# Add label to the X-axis\nplt.xlabel(\"Quantity\")\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We accurately remove the abnormally high value (80995). Then let's look at the chart again.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Remove abnormally high value\ndf_full = df_full.query('quantity < 80000')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "df_full.describe()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Set the size of the boxplot\nplt.figure(figsize=(10, 6))\n\n# Plot a horizontal boxplot\nplt.boxplot(x = df_full['quantity'],  # quantity data\n            vert=False,     # horizontal boxplot\n            showmeans=True, # show mean on the plot\n            meanline=True,  # show mean as a line\n            patch_artist=True,  # fill the boxplot with color\n            # set outliers color to red\n            flierprops=dict(markerfacecolor='red')) \n\n# Add title to the plot\nplt.title(\"Outlier Analysis of Quantity\")\n\n# Add label to the X-axis\nplt.xlabel(\"Quantity\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We still see high values ​​in the quantity column, but since buyers are not only individuals, but also wholesale stores, we will leave these values, since they may apply specifically to wholesale buyers.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Cheking the price column",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Set the size of the boxplot\nplt.figure(figsize=(10, 6))\n\n# Plot a horizontal boxplot\nplt.boxplot(x = df_full['price'],  # price data\n            vert=False,     # horizontal boxplot\n            showmeans=True, # show mean on the plot\n            meanline=True,  # show mean as a line\n            patch_artist=True,  # fill the boxplot with color\n            # set outliers color to red\n            flierprops=dict(markerfacecolor='red')) \n\n# Add title to the plot\nplt.title(\"Outlier Analysis of Price\")\n\n# Add label to the X-axis\nplt.xlabel(\"Price\")\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Filter rows where price > 10000\nprice_over_100 = df_full.query('price > 10000')\n\n# Find unique values in the 'entry' column in the filtered dataset\nunique_entries = price_over_100['entry'].unique()\n\n# Display unique entries\nprint(unique_entries)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Nothing abnormally high was recorded. Just expensive goods",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Calculate the percentage of deleted data during preprocessing\nshare_of_deleted_data = (1 - (len(df_full) / len(df_full_raw))) * 100\nprint(f\"Percentage of deleted rows: {share_of_deleted_data:.4f}%\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Exploring Categorical Columns",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Define a function to analyze columns\ndef analyze_column(df_full, column_name):\n    print(f\"\\nColumn Analysis: {column_name}\")\n    print(f\"Number of unique values: {df_full[column_name].nunique()}\")\n    print(f\"Number of missing values: {df_full[column_name].isnull().sum()}\")\n    print(f\"Examples of unique values:\\n{df_full[column_name].unique()[:10]}\")\n    if df_full[column_name].dtype == 'object':\n        print(f\"Number of values with spaces: {df_full[column_name].str.contains(' ', regex=False).sum()}\")\n    elif df_full[column_name].dtype in ['int64', 'float64']:\n        print(f\"Minimum value: {df_full[column_name].min()}\")\n        print(f\"Maximum value: {df_full[column_name].max()}\")\n        print(f\"Mean value: {df_full[column_name].mean()}\")\n    print(\"\\n\")\n\n# Analyze columns\nanalyze_column(df_full, 'order_id')\nanalyze_column(df_full, 'customer_id')\nanalyze_column(df_full, 'name_clust')\nanalyze_column(df_full, 'entry_id')\nanalyze_column(df_full, 'country_id')\n\n# Create a column 'price_of_order' where the total cost of each item in the dataset is calculated\ndf_full['price_of_order'] = df_full['price'] * df_full['quantity']\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Let's calculate by month the number of days in which there were no sales. Let's choose a period for analysis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Convert date columns to datetime format\ndf_full['entry_date'] = pd.to_datetime(df_full['entry_date'])\n\n# Create new columns for year, month, day, hour, and day of the week\ndf_full['year'] = df_full['entry_date'].dt.year\ndf_full['month'] = df_full['entry_date'].dt.month\ndf_full['day'] = df_full['entry_date'].dt.day\ndf_full['hour'] = df_full['entry_date'].dt.hour\ndf_full['day_of_week'] = df_full['entry_date'].dt.day_name()\n\n# Count the number of orders for each day of the week\norders_by_day = df_full.groupby('day_of_week')['order_id'].nunique().reindex(\n    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n)\n\n# Visualize the results\nplt.figure(figsize=(10, 6))\norders_by_day.plot(kind='bar', color='skyblue')\n\n# Add labels and title\nplt.xlabel('Day of the Week')\nplt.ylabel('Number of Orders')\nplt.title('Number of Orders by Day of the Week')\n\n# Add gridlines\nplt.grid(axis='y')\n\n# Display the plot\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "We see that there are no sales on Tuesdays. I take this as an error in the dataset. Most orders are on Friday-Sunday.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Group by year and month, and count the number of unique days\ngroup_by_year_month = df_full.groupby(['year', 'month'])['day'].nunique().reset_index()\n\n# Rename the 'day' column to 'unique_days'\ngroup_by_year_month.rename(columns={'day': 'unique_days'}, inplace=True)\n\n# Display the result\nprint(group_by_year_month)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Так как присутсвует неполный 12-ый месяц 2019 года, то будем изучасть период такой: 1.1.2019 - 30.11.2019",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Filter data for the period from January 1, 2019 to November 30, 2019\nstart_date = '2019-01-01'\nend_date = '2019-11-30'\n\ndf_full = df_full[(df_full['entry_date'] >= start_date) & (df_full['entry_date'] <= end_date)]",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Step 3: Calculation of metrics",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Group by days and count the number of unique customers and orders\ngroup_by_days_unique = df_full.groupby(['day'])[['customer_id', 'order_id']].nunique().reset_index()\n\n# Rename the 'customer_id' and 'order_id' columns to 'unique_customers' and 'unique_orders'\ngroup_by_days_unique.rename(columns={'customer_id': 'unique_customers', 'order_id': 'unique_orders'}, inplace=True)\n\n# Display the result\ngroup_by_days_unique",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Number of unique customers and orders by day",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Plotting the line graph\nplt.figure(figsize=(10, 6))\n\n# Line for unique customers\nplt.plot(group_by_days_unique['days'], group_by_days_unique['unique_customers'], marker='o', label='Unique Customers')\n\n# Line for unique orders\nplt.plot(group_by_days_unique['days'], group_by_days_unique['unique_orders'], marker='x', label='Unique Orders')\n\n# Adding labels and title\nplt.xlabel('Day of Purchase')\nplt.ylabel('Number of Unique Customers and Orders')\nplt.title('Number of Unique Customers and Orders in the period 01-2019 - 11-2019')\n\n# Adding grid\nplt.grid(True)\n\n# Adding legend\nplt.legend()\n\n# Display the plot\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The largest number of unique buyers is at the beginning of the month; by the end of the month there is a noticeable decline, but overall the trend is quite flat. The same trend applies to orders (repeats the curve of unique customers)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Number of unique customers and orders by hour",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Grouping by hours and counting unique customers and orders\ngroup_by_hours_unique = df_full.groupby(['hours'])[['customer_id', 'order_id']].nunique().reset_index()\n\n# Renaming columns 'customer_id' and 'order_id' to 'unique_customers' and 'unique_orders'\ngroup_by_hours_unique.rename(columns={'customer_id': 'unique_customers', 'order_id': 'unique_orders'}, inplace=True)\n\n# Displaying the grouped data\ngroup_by_hours_unique",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Plotting the line graph\nplt.figure(figsize=(10, 6))\n\n# Line for unique customers\nplt.plot(group_by_hours_unique['hours'], group_by_hours_unique['unique_customers'], marker='o', label='Unique Customers')\n\n# Line for unique orders\nplt.plot(group_by_hours_unique['hours'], group_by_hours_unique['unique_orders'], marker='x', label='Unique Orders')\n\n# Adding labels and title\nplt.xlabel('Hour of Purchase')\nplt.ylabel('Number of Unique Customers and Orders')\nplt.title('Number of Unique Customers and Orders in the period 01-2019 - 11-2019')\n\n# Adding grid\nplt.grid(True)\n\n# Adding legend\nplt.legend()\n\n# Displaying the plot\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The peak of orders occurs at lunchtime. In the morning (6-8) and evening (18-20) activity is minimal",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "### Average revenue by month",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": "# Grouping by months and calculating the average price of order for each month\ngroup_by_avg_price_month = df_full.groupby(['month'])['price_of_order'].mean().reset_index()\n\n# Grouping by months and counting the number of unique customers\ngroup_by_customer_unique = df_full.groupby(['month'])['customer_id'].nunique().reset_index()\n\n# Renaming column 'price_of_order' to 'average_price_of_order'\ngroup_by_avg_price_month.rename(columns={'price_of_order': 'average_price_of_order'}, inplace=True)\n\n# Renaming column 'customer_id' to 'unique_customer'\ngroup_by_customer_unique.rename(columns={'customer_id': 'unique_customer'}, inplace=True)\n\n# Plotting the line graph for average revenue per month\nplt.figure(figsize=(10, 6))\nplt.plot(group_by_avg_price_month['month'], group_by_avg_price_month['average_price_of_order'], marker='o')\n\n# Adding labels and title\nplt.xlabel('Month')\nplt.ylabel('Average Revenue')\nplt.title('Average Revenue of the Online Store in the period 01-2019 - 11-2019')\n\n# Adding grid\nplt.grid(True)\n\n# Displaying the plot\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Studying the graph, we can come to the conclusion that there is no seasonality in the sales of the online store. Average revenue is in the range of 1600 - 2100 units of money, where the peak was the month of August.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Plotting the line graph for unique customers per month\nplt.figure(figsize=(10, 6))\nplt.plot(group_by_customer_unique['month'], group_by_customer_unique['unique_customer'], marker='o')\n\n# Adding labels and title\nplt.xlabel('Month')\nplt.ylabel('Unique Customers Count')\nplt.title('Unique Customers of the Online Store in the period 01-2019 - 11-2019')\n\n# Adding grid\nplt.grid(True)\n\n# Displaying the plot\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The number of unique customers grew throughout 2019, with a peak in November (1,600 unique customers). Again, there is no point in talking about seasonality.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Sticky Factor for 2nd and 3rd quarters of 2019",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Filtering data for the 2nd and 3rd quarters of 2019\ndf_q2 = df_full[(df_full['entry_date'] >= '2019-04-01') & (df_full['entry_date'] <= '2019-06-30')]\ndf_q3 = df_full[(df_full['entry_date'] >= '2019-07-01') & (df_full['entry_date'] <= '2019-09-30')]\n\n# Finding unique customers in each quarter\nunique_customers_q2 = set(df_q2['customer_id'].unique())\nunique_customers_q3 = set(df_q3['customer_id'].unique())\n\n# Finding the intersection of customers who made purchases in both quarters\ncommon_customers = unique_customers_q2.intersection(unique_customers_q3)\n\n# Calculating stickiness factor for the 2nd quarter\nstickiness_factor_q2 = len(common_customers) / len(unique_customers_q2)\n\n# Calculating stickiness factor for the 3rd quarter\nstickiness_factor_q3 = len(common_customers) / len(unique_customers_q3)\n\nprint(f'Sticky Factor for the 2nd quarter of 2019: {stickiness_factor_q2:.2f}')\nprint(f'Sticky Factor for the 3rd quarter of 2019: {stickiness_factor_q3:.2f}')\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Dividing clients into profiles",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Creating aggregated profile for each customer\ncustomer_profile = df_full.groupby('customer_id').agg(\n    num_orders=('order_id', 'nunique'),                  # Number of unique orders\n    first_order_date=('entry_date', 'min'),              # First order date\n    last_order_date=('entry_date', 'max'),               # Last order date\n    total_quantity=('quantity', 'sum'),                  # Total quantity purchased\n    total_amount=('price_of_order', 'sum'),              # Total amount spent\n    avg_order_price=('price_of_order', 'mean')           # Average order price\n).reset_index()\n\n# Calculating additional metrics\ncustomer_profile['avg_quantity_per_order'] = customer_profile['total_quantity'] / customer_profile['num_orders']\n\n# Sorting by number of orders in descending order\ncustomer_profile = customer_profile.sort_values(by='num_orders', ascending=False)\n\n# Displaying a random sample of 10 rows from the customer profile\ncustomer_profile.sample(10)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Dividing customers into returning and non-returning customers based on the presence of repeat purchases",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Dividing customers into returning and non-returning\nreturning_customers = customer_profile[customer_profile['num_orders'] > 1]\nnon_returning_customers = customer_profile[customer_profile['num_orders'] == 1]\n\n# Calculating average metrics for each group\nreturning_avg = returning_customers.mean()\nnon_returning_avg = non_returning_customers.mean()\n\nprint(\"Average metrics for returning customers:\")\nprint(returning_avg[['num_orders', 'total_quantity', 'total_amount', 'avg_order_price', 'avg_quantity_per_order']])\n\nprint(\"\\nAverage metrics for non-returning customers:\")\nprint(non_returning_avg[['total_quantity', 'total_amount', 'avg_order_price', 'avg_quantity_per_order']])\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Returning customers logically have a much higher average average. What is interesting is that the average order price was almost the same in both groups (3003 units of money for returnable ones versus 2835 units of money for non-refundable ones), as well as the average number of goods in the order (167 units for returnable ones versus 158 units for non-refundable ones). non-refundable). On average, return customers have 6 orders per study period.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Step 4: RFM customer segmentation",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Current date (for recency calculation)\ncurrent_date = pd.to_datetime('2024-06-06')\n\n# Calculating RFM metrics\nrfm_df = df_full.groupby('customer_id').agg({\n    'entry_date': lambda x: (current_date - x.max()).days,\n    'order_id': 'count',\n    'price_of_order': 'sum'\n}).reset_index()\n\nrfm_df.columns = ['customer_id', 'Recency', 'Frequency', 'Monetary']\n\nrfm_df.head()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "### Evaluation of the resulting groups and recommendations for business on interaction with segments",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Define a function to assign scores based on quantiles\ndef rfm_score(x, quantiles, reverse=False):\n    if reverse:\n        return pd.qcut(x, q=4, labels=False, duplicates='drop')\n    else:\n        return pd.qcut(x, q=4, labels=False, duplicates='drop')\n\n# Calculate quantiles for RFM metrics\nquantiles = rfm_df.quantile(q=[0.25, 0.5, 0.75])\n\n# Assign R, F, M scores\nrfm_df['R_score'] = rfm_score(rfm_df['Recency'], quantiles['Recency'], reverse=True).astype(int)\nrfm_df['F_score'] = rfm_score(rfm_df['Frequency'], quantiles['Frequency']).astype(int)\nrfm_df['M_score'] = rfm_score(rfm_df['Monetary'], quantiles['Monetary']).astype(int)\n\n# Calculate RFM segment\nrfm_df['RFM_segment'] = rfm_df['R_score'].astype(str) + rfm_df['F_score'].astype(str) + rfm_df['M_score'].astype(str)\n\n# Calculate RFM score\nrfm_df['RFM_score'] = rfm_df[['R_score', 'F_score', 'M_score']].sum(axis=1)\n\nrfm_df.head()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Segment description\nsegment_summary = rfm_df.groupby('RFM_segment').agg({\n    'Recency': 'mean',\n    'Frequency': 'mean',\n    'Monetary': 'mean',\n    'customer_id': 'count'\n}).reset_index()\n\nsegment_summary.columns = ['RFM_segment', 'Avg_Recency', 'Avg_Frequency', 'Avg_Monetary', 'Count_Customers']\n\nsegment_summary\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Visualizing RFM segments\nplt.figure(figsize=(10, 6))\nplt.bar(segment_summary['RFM_segment'], segment_summary['Count_Customers'], color='skyblue')\nplt.xlabel('RFM Segment')\nplt.ylabel('Number of Customers')\nplt.title('Distribution of Customers across RFM Segments')\nplt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True)) \nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Based on the analysis, the following recommendations can be formulated:\n\n1) Most valuable customers (RFM_score 9-12):\n\n-Maintain a high level of service.\n\n-Offer exclusive offers and loyalty programs.\n\n2) Clients on the verge of leaving (RFM_score 1-4):\n\n-Develop retention campaigns.\n\n-Offer discounts and special promotions.\n\n3) Clients with average activity (RFM_score 5-8): -Stimulate repeat purchases.\n\n-Offer personalized recommendations.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Step 5: Testing statistical hypotheses",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Comparison of the share of returning and non-returning customers for the second and third quarters of 2019",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Filtering data for the second and third quarters of 2019\nsecond_quarter_2019 = df_full[(df_full['entry_date'] >= '2019-04-01') & (df_full['entry_date'] <= '2019-06-30')]\nthird_quarter_2019 = df_full[(df_full['entry_date'] >= '2019-07-01') & (df_full['entry_date'] <= '2019-09-30')]\n\n# Defining returning customers (customers who made more than one order)\ndef returning_customers(df_full):\n    return df_full.groupby('customer_id').filter(lambda x: len(x) > 1)['customer_id'].nunique()\n\n# Defining non-returning customers (customers who made only one order)\ndef non_returning_customers(df_full):\n    return df_full.groupby('customer_id').filter(lambda x: len(x) == 1)['customer_id'].nunique()\n\n# Counting the number of returning and non-returning customers in the second and third quarters\nreturning_2q_2019 = returning_customers(second_quarter_2019)\nnon_returning_2q_2019 = non_returning_customers(second_quarter_2019)\nreturning_3q_2019 = returning_customers(third_quarter_2019)\nnon_returning_3q_2019 = non_returning_customers(third_quarter_2019)\n\nprint(f'2Q2019: Returning: {returning_2q_2019}, Non-returning: {non_returning_2q_2019}')\nprint(f'3Q2019: Returning: {returning_3q_2019}, Non-returning: {non_returning_3q_2019}')\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Contingency table\ncontingency_table = [\n    [returning_2q_2019, non_returning_2q_2019],\n    [returning_3q_2019, non_returning_3q_2019]\n]\n\n# Chi-square test\nchi2, p, dof, ex = stats.chi2_contingency(contingency_table)\n\nprint(f'Chi2: {chi2}, p-value: {p}')\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The p-value (0.4756) significantly exceeds the significance level of 0.05, which indicates that there are no statistically significant differences in the shares of returning and non-returning customers between the second and third quarters of 2019.\n\nBased on the chi-square test performed, there is no sufficient reason to reject the null hypothesis. This means that the shares of returning and non-returning customers in the second and third quarters of 2019 are not statistically different. Thus, we can conclude that customer repayment behavior has not changed between these two quarters.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Comparison of average receipts in countries with country_id equal to 3, 6 and 24",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Filter data by country_id\ncountries = [3, 6, 24]\ncountry_data = df_full[df_full['country_id'].isin(countries)]\n\n# Group by country_id and calculate average order price\ncountry_avg_check = country_data.groupby('country_id')['price_of_order'].mean().reset_index()\ncountry_avg_check",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Applying ANOVA statistical test\ncountry3 = country_data[country_data['country_id'] == 3]['price_of_order']\ncountry6 = country_data[country_data['country_id'] == 6]['price_of_order']\ncountry24 = country_data[country_data['country_id'] == 24]['price_of_order']\n\nf_stat, p_val = stats.f_oneway(country3, country6, country24)\n\nprint(f'F-statistic: {f_stat}, p-value: {p_val}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The p-value (2.2169939757520933e-170) is significantly less than the significance level of 0.05, which indicates that there are statistically significant differences between average receipts in countries with country_id 3, 6 and 24.\n\nBased on the ANOVA test, we can conclude that average checks in countries with country_id 3, 6 and 24 are statistically significantly different. This means that the average purchase amounts differ between these countries, which may indicate differences in purchasing behavior or economic conditions between these countries.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Own hypothesis - Customers who have made purchases worth more than 1800 units of money have a greater chance of becoming returning customers",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Filter customers by purchase amount\nhigh_spenders = rfm_df[rfm_df['Monetary'] > 1800]\nlow_spenders = rfm_df[rfm_df['Monetary'] <= 1800]\n\n# Calculate returning customers\nhigh_returning = high_spenders[high_spenders['Frequency'] > 1].shape[0]\nlow_returning = low_spenders[low_spenders['Frequency'] > 1].shape[0]\n\n# Calculate total number of customers\nhigh_total = high_spenders.shape[0]\nlow_total = low_spenders.shape[0]\n\nprint(f'Returning customers with spending > 1800 units of money: {high_returning}, Total customers with spending > 1800 units of money: {high_total}')\nprint(f'Returning customers with spending <= 1800 units of money: {low_returning}, Total customers with spending <= 1800 units of money: {low_total}')\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Contingency table\ncount = [high_returning, low_returning]\nnobs = [high_total, low_total]\n\n# Proportions test\nstat, pval = proportions_ztest(count, nobs)\n\nprint(f'Z-statistic: {stat}, p-value: {pval}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The p-value (1.4632831908230165e-94) is significantly less than the 0.05 significance level, indicating that there are statistically significant differences between the proportions of returning customers among those who spent more than 1800 and those who spent less.\n\nBased on the proportion test conducted, it can be concluded that customers who made purchases of more than 1800 are significantly more likely to become returning customers compared to those who spent less. This means that a customer's total spending affects their likelihood to return and make repeat purchases.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "General conclusion:\n\nBased on the analysis of online store data, the following conclusions can be drawn:\n\n1) Trends in customer activity and orders:\n\nThere is an increase in the number of unique clients at the beginning of the month, followed by a gradual decrease towards the end of the month. However, the overall trend remains quite stable. The peak of orders occurs at lunchtime, and in the morning and evening activity is minimal.\n\n2) Seasonality in sales:\n\nThere is no pronounced seasonality in the sales graph. Average revenue remains relatively stable, with a peak in the month of August.\n\n3) Number of unique clients:\n\nThere has been an increase in the number of unique customers throughout 2019, peaking in November. Again, seasonality is not observed.\n\n4) Sticky Factor:\n\nSticky Factor for the 2nd and 3rd quarters of 2019 is 0.6 and 0.56, respectively, which indicates fairly good customer retention.\n\n5) Analysis of returning customers:\n\nReturning customers have higher averages, such as average order price and average number of items per order, compared to non-returning customers. On average, returning customers had more orders during the study period.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}